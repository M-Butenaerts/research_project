{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full notebook\n",
    "This notebook has the whole process put together:\n",
    "1. Loading dataset\n",
    "2. Defining grammar\n",
    "3. Searching through the grammar and evaluating candidate pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following if not all packages are added.\n",
    "\n",
    "# import Pkg\n",
    "# using Pkg\n",
    "# Pkg.add(\"HTTP\")\n",
    "# Pkg.add(\"JSON\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"OpenML\")\n",
    "# Pkg.add(\"DataFrames\") \n",
    "# Pkg.add(\"CSV\") \n",
    "# Pkg.add(\"Suppressor\")\n",
    "# Pkg.add(\"StatsBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.Pipelines: Pipeline, FeatureUnion\n",
    "using ScikitLearn.CrossValidation: cross_val_score\n",
    "using XGBoost\n",
    "using Revise\n",
    "using Random\n",
    "using Statistics: mean\n",
    "using ExprRules: get_executable\n",
    "using Suppressor\n",
    "using Random\n",
    "using Dates\n",
    "\n",
    "include(\"./Herb.jl/src/Herb.jl\")\n",
    "include(\"./Herb.jl/HerbGrammar.jl/src/HerbGrammar.jl\")\n",
    "include(\"./Herb.jl/HerbData.jl/src/HerbData.jl\")\n",
    "include(\"./Herb.jl/HerbEvaluation.jl/src/HerbEvaluation.jl\")\n",
    "include(\"./Herb.jl/HerbConstraints.jl/src/HerbConstraints.jl\")\n",
    "include(\"./Herb.jl/HerbSearch.jl/src/HerbSearch.jl\")\n",
    "include(\"helper.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the sk-learn functions\n",
    "@sk_import decomposition: (PCA)\n",
    "@sk_import preprocessing: (StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Binarizer, PolynomialFeatures)\n",
    "@sk_import feature_selection: (VarianceThreshold, SelectKBest, SelectPercentile, SelectFwe, RFE)\n",
    "@sk_import tree: (DecisionTreeClassifier)\n",
    "@sk_import ensemble: (RandomForestClassifier, GradientBoostingClassifier)\n",
    "@sk_import linear_model: (LogisticRegression)\n",
    "@sk_import neighbors: (NearestNeighbors)\n",
    "@sk_import svm: (LinearSVC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris: 61, Seeds: 1499, Blood transfusion: 1464, Monks: 334, Diabetes: 37, ilpd: 1480\n",
    "\n",
    "# load dataset\n",
    "# dataset = get_dataset(61)\n",
    "\n",
    "# it does not work for the seeds table dataset!\n",
    "dataset = get_dataset(1499)\n",
    "\n",
    "# does not work either on dataset 1464!\n",
    "\n",
    "# shuffle the dataset\n",
    "dataset_shuffled = dataset[shuffle(1:end), :]\n",
    "\n",
    "# split into train and test sets (90:10)\n",
    "split_index = floor(Int, size(dataset_shuffled, 1) * 0.90)\n",
    "train_df = dataset_shuffled[1:split_index, :]\n",
    "test_df = dataset_shuffled[split_index+1:end, :]\n",
    "\n",
    "# show first five entries\n",
    "first(dataset_shuffled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show metadata\n",
    "print(\"size: \", size(dataset_shuffled))\n",
    "describe(dataset_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and labels\n",
    "train_X = train_df[:, 1:end-1]\n",
    "train_Y = train_df[:, end]\n",
    "test_X = test_df[:, 1:end-1]\n",
    "test_Y = test_df[:, end]\n",
    "\n",
    "# print ratio train/test\n",
    "ratio = trunc(Int, 10.0 * (size(train_df)[1] / (size(train_df)[1] + size(test_df)[1])))\n",
    "print(\"train:test ratio = \", ratio , \":\", (10-ratio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Herb.HerbGrammar.@cfgrammar begin\n",
    "\n",
    "    # this is the version with multiple classifiers possible\n",
    "    # START   = CLASSIF | sequence(PRE, CLASSIF)\n",
    "    # PRE     = PREPROC | FSELECT | sequence(PRE, PRE) | parallel(BRANCH, BRANCH)\n",
    "    # BRANCH  = PRE | CLASSIF | sequence(PRE, CLASSIF) \n",
    "\n",
    "    # this is the version with only one classifier\n",
    "    START   = Pipeline([CLASSIF]) | Pipeline([PRE, CLASSIF])\n",
    "    PRE     = PREPROC | FSELECT | (\"seq\", Pipeline([PRE, PRE]))  | (\"par\", FeatureUnion([PRE, PRE])) \n",
    "\n",
    "    # preprocessing functions\n",
    "    PREPROC =   \n",
    "        (\"StandardScaler\" * string(rand(Int)), StandardScaler()) |\n",
    "        (\"RobustScaler\", RobustScaler()) |\n",
    "        (\"MinMaxScaler\", MinMaxScaler()) |\n",
    "        (\"MaxAbsScaler\", MaxAbsScaler()) |\n",
    "        (\"PCA\", PCA()) |\n",
    "        (\"Binarizer\", Binarizer()) |\n",
    "        (\"PolynomialFeatures\", PolynomialFeatures())\n",
    "\n",
    "    # feature selection functions\n",
    "    FSELECT =  \n",
    "        (\"VarianceThreshold\", VarianceThreshold()) |\n",
    "        # (\"SelectKBest\",  SelectKBest()) |\n",
    "        (\"SelectPercentile\",  SelectPercentile()) |\n",
    "        (\"SelectFwe\",  SelectFwe()) |\n",
    "        (\"Recursive Feature Elimination\",  RFE(LinearSVC())) \n",
    "\n",
    "    # classifiers\n",
    "    CLASSIF =\n",
    "        (\"DecisionTree\", DecisionTreeClassifier()) |\n",
    "        (\"RandomForest\", RandomForestClassifier()) |\n",
    "        (\"Gradient Boosting Classifier\", GradientBoostingClassifier()) |\n",
    "        (\"LogisticRegression\", LogisticRegression()) |\n",
    "        (\"NearestNeighborClassifier\", NearestNeighbors())\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function insert_name_indexes(p)\n",
    "    p_start = \"\"\n",
    "    i = 1\n",
    "    while i <= 100\n",
    "        try\n",
    "            p = replace(p, \"\"\"\",\"\"\" => string(i)*\"\"\"\",\"\"\", count=1)\n",
    "            p_split = split(p, string(i) * \"\"\"\", \"\"\")\n",
    "            p_start *= p_split[1] * string(i) * \"\"\"\", \"\"\"\n",
    "            p = p_split[2]\n",
    "            i += 1\n",
    "        catch\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return split(p_start, string(i))[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_random_pipeline(grammar, max_depth, start_symbol)\n",
    "    # all pipelines that can be assembled in max_depth steps\n",
    "    cfe = Herb.HerbSearch.ContextFreeEnumerator(grammar, max_depth, :START)\n",
    "    cfe_size = deepcopy(cfe)\n",
    "    \n",
    "    # find size\n",
    "    size = 0\n",
    "    for pipeline in cfe_size\n",
    "        size += 1\n",
    "    end\n",
    "\n",
    "    # Find start program\n",
    "    ret_pipeline = nothing\n",
    "    c = 0\n",
    "    i = abs(rand(Int) % size)\n",
    "    for pipeline in cfe\n",
    "        if c == i\n",
    "            ret_pipeline = pipeline\n",
    "            break\n",
    "        end\n",
    "        c = c + 1\n",
    "    end\n",
    "    return ret_pipeline\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vlns(grammar, enumeration_depth)\n",
    "    current_program = get_random_pipeline(grammar, 4, :START)\n",
    "    current_cost = 1.1\n",
    "    i = 0\n",
    "    not_improved_counter = 0\n",
    "    max_seconds = 30\n",
    "    max_time = Dates.Millisecond(max_seconds * 1000)\n",
    "    t_start = now()\n",
    "    while i < 100\n",
    "        t_now = now()\n",
    "        if t_now - t_start > max_time\n",
    "            break\n",
    "        end\n",
    "        println(\"iteration: \", i)\n",
    "        previous_cost = current_cost\n",
    "        current_program, current_cost = find_best_neighbour_in_neighbourhood(current_program, grammar, enumeration_depth, t_start, max_time)\n",
    "\n",
    "        if current_cost == previous_cost\n",
    "            not_improved_counter += 1\n",
    "            if not_improved_counter == 10\n",
    "                println(\"stopping because hasn't inproved in 10 iterations\")\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            not_improved_counter = 0\n",
    "        end\n",
    "        if current_cost == 0.0\n",
    "            println(\"stopping because cost is 0.0\")\n",
    "            break\n",
    "        end\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    println(\"final program: \", current_program)\n",
    "    println(\"final cost: \", current_cost)\n",
    "    return current_program, current_cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_best_neighbour_in_neighbourhood(current_program, grammar, enumeration_depth, t_start, max_time)\n",
    "    # println(\"Start vlns iteration\")\n",
    "    # println(\"current_program = \", current_program)\n",
    "    # println(\"current_cost = \", pipeline_cost_function(eval(Herb.HerbSearch.rulenode2expr(current_program, grammar)), train_X, train_Y, test_X, test_Y))\n",
    "    # 1. Construct neighbourhood\n",
    "    neighbourhood_node_loc, dict = Herb.HerbSearch.constructNeighbourhoodRuleSubset(current_program, grammar)\n",
    "    replacement_expressions = Herb.HerbSearch.enumerate_neighbours_propose(enumeration_depth)(current_program, \n",
    "                                                                                                neighbourhood_node_loc, \n",
    "                                                                                                grammar,\n",
    "                                                                                                5, # max_depth = max depth of pipeline, depth of subprogram is bound by this\n",
    "                                                                                                dict)\n",
    "\n",
    "    # 2. Find best neighbour\n",
    "    best_program = deepcopy(current_program)\n",
    "    pipeline = eval(Meta.parse(insert_name_indexes(string(Herb.HerbSearch.rulenode2expr(best_program, grammar)))))\n",
    "    best_program_cost = pipeline_cost_function(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "    possible_program = current_program\n",
    "    neighbours_tried = 0\n",
    "    for replacement_expression in replacement_expressions\n",
    "        t_now = now()\n",
    "        if t_now - t_start > max_time\n",
    "            println(\"timelimit reached\")\n",
    "            break\n",
    "        end\n",
    "        if (neighbours_tried % 10) == 0\n",
    "            println(\"tried \", neighbours_tried, \" neighbours\")\n",
    "            if neighbours_tried == 30\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # change current_program to one of its neighbours \n",
    "        if neighbourhood_node_loc.i == 0\n",
    "            possible_program = replacement_expression\n",
    "        else\n",
    "            neighbourhood_node_loc.parent.children[neighbourhood_node_loc.i] = replacement_expression\n",
    "        end\n",
    "        pipeline = eval(Meta.parse(insert_name_indexes(string(Herb.HerbSearch.rulenode2expr(possible_program, grammar)))))\n",
    "        possible_program_cost = pipeline_cost_function(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "        # println(\"possible pipeline: \", possible_program)#Herb.HerbSearch.rulenode2expr(pl, grammar)) \n",
    "        # println(\"possible pipeline cost: \", possible_program_cost)\n",
    "        if possible_program_cost < best_program_cost\n",
    "            # println(\"Found a better pipeline!: \", possible_program)\n",
    "            best_program = deepcopy(current_program)\n",
    "            best_program_cost = possible_program_cost        \n",
    "        end\n",
    "        neighbours_tried += 1\n",
    "    end\n",
    "    # println(\"Finished vlns iteration\")\n",
    "    # println(\"best_program = \", best_program)\n",
    "    # println(\"best_cost = \", best_program_cost)\n",
    "    # println()\n",
    "    return best_program, best_program_cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlns(grammar, 3) # enumeration_depth = max depth of subprogram that is being replaced, regardless of the NodeLoc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fits the pipeline to the training set and measures the accuracy on test set.\n",
    "input:  pipeline, train_X, train_Y, test_X, test_Y\n",
    "output: accuracy of pipeline\n",
    "\"\"\"\n",
    "function evaluate_pipeline(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "\n",
    "    # # this gives the following warning often, so it is suppressed for now.\n",
    "    # # ConvergenceWarning: lbfgs failed to converge\n",
    "    @suppress_err begin\n",
    "        try\n",
    "            # fit the pipeline\n",
    "            # print(pipeline)\n",
    "            # print(\" - \")\n",
    "            model = ScikitLearn.fit!(pipeline, Matrix(train_X), Array(train_Y))\n",
    "\n",
    "            # make predictions\n",
    "            predictions = ScikitLearn.predict(model, Matrix(test_X))\n",
    "\n",
    "            # measure the accuracy\n",
    "            accuracy = mean(predictions .== test_Y)\n",
    "            return accuracy\n",
    "        catch e\n",
    "            # println(\"Caught error [in evaluate_pipeline()]: \", e)\n",
    "            return 0.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains the pipeline and returns 1-accuracy\"\"\"\n",
    "function pipeline_cost_function(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "    return 1.0 - evaluate_pipeline(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function enumerates the grammar and finds the best pipeline. \"\"\"\n",
    "function find_best_pipeline_with_bfs_search(grammar, train_X, train_Y, test_X, test_Y, search_depth)\n",
    "    best_accuracy = -1.0\n",
    "    best_pipeline = nothing\n",
    "\n",
    "    # enumerate the gramamar\n",
    "    enumerator = Herb.HerbSearch.ContextFreeEnumerator(grammar, search_depth, :START)\n",
    "    for rule in enumerator\n",
    "        try\n",
    "            # get pipeline and calculate accuracy\n",
    "            pipeline = eval(Herb.HerbSearch.rulenode2expr(rule, grammar))\n",
    "            accuracy = evaluate_pipeline(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "\n",
    "            # update best pipeline\n",
    "            if (accuracy > best_accuracy) \n",
    "                best_accuracy = accuracy\n",
    "                best_pipeline = pipeline\n",
    "            end\n",
    "            \n",
    "            # print accuracy of pipeline\n",
    "            print(\"\\n accuracy: \", round(accuracy, digits=2), \" by \", string(pipeline))\n",
    "        catch\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    return (best_accuracy, best_pipeline)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best pipeline and accuracy with depth 2\n",
    "(best_accuracy, best_pipeline) = find_best_pipeline_with_bfs_search(grammar, train_X, train_Y, test_X, test_Y, 2)\n",
    "\n",
    "# print result\n",
    "println(\"\\n\\nBest pipeline: \", round(best_accuracy, digits=2))\n",
    "print(best_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline([(\"DecisionTree\", DecisionTreeClassifier())])\n",
    "pipeline_cost_function(test_pipeline, train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function enumerates the grammar and finds the best pipeline. \"\"\"\n",
    "function find_best_pipeline_with_vlsn(grammar, train_X, train_Y, test_X, test_Y, search_depth)\n",
    "    best_accuracy = -1.0\n",
    "    best_pipeline = nothing\n",
    "\n",
    "    # enumerate the gramamar\n",
    "    # enumerator = Herb.HerbSearch.ContextFreeEnumerator(grammar, search_depth, :START)\n",
    "    enumerator = Herb.HerbSearch.get_vlsn_enumerator(grammar, [], search_depth, :START, pipeline_cost_function)\n",
    "    for rule in enumerator\n",
    "        try\n",
    "            # get pipeline and calculate accuracy\n",
    "            pipeline = eval(Herb.HerbSearch.rulenode2expr(rule, grammar))\n",
    "            accuracy = evaluate_pipeline(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "\n",
    "            # update best pipeline\n",
    "            if (accuracy > best_accuracy) \n",
    "                best_accuracy = accuracy\n",
    "                best_pipeline = pipeline\n",
    "            end\n",
    "            \n",
    "            # print accuracy of pipeline\n",
    "            print(\"\\n accuracy: \", round(accuracy, digits=2), \" by \", string(pipeline))\n",
    "        catch\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    return (best_accuracy, best_pipeline)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_pipeline_with_vlsn(grammar, train_X, train_Y, test_X, test_Y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p_cost_f(pipeline)\n",
    "    # pipeline = eval(pipeline)\n",
    "    return pipeline_cost_function(pipeline, train_X, train_Y, test_X, test_Y)\n",
    "end\n",
    "\n",
    "enumerator = Herb.HerbSearch.get_vlsn_enumerator(grammar, [], 2, :START, p_cost_f)\n",
    "# Herb.HerbSearch.get_mh_enumerator()\n",
    "c = 0\n",
    "for rule in enumerator\n",
    "    println(Herb.HerbSearch.rulenode2expr(rule, grammar))\n",
    "    c = c + 1\n",
    "    if c == 20\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
